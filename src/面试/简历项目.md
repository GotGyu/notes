# linux_monitor

## 项目背景

1. 做这个系统的原因：
2. 对接的需求：

## docker相关 

 [](./docker使用.md)

## 设计思路

<pre style="max-height: 400px; overflow: auto; border: 1px solid #ddd; padding: 10px;">
stress <options>
# 参数
-c, --cpu N  # 产生 N 个进程，每个进程都反复不停的计算随机数的平方根
-i, --io N  # 产生 N 个进程，每个进程反复调用 sync() 将内存上的内容写到硬盘上
-m, --vm N  # 产生 N 个进程，每个进程不断分配和释放内存
    --vm-bytes B  # 指定分配内存的大小
    --vm-stride B  # 不断的给部分内存赋值，让 COW(Copy On Write)发生
    --vm-hang N  # 指示每个消耗内存的进程在分配到内存后转入睡眠状态 N 秒，然后释放内存，一直重复执行这个过程
    --vm-keep  # 一直占用内存，区别于不断的释放和重新分配(默认是不断释放并重新分配内存)
-d, --hadd N  # 产生 N 个不断执行 write 和 unlink 函数的进程(创建文件，写入内容，删除文件)
    --hadd-bytes B  # 指定文件大小
-t, --timeout N  # 在 N 秒后结束程序        
--backoff N  # 等待N微妙后开始运行
-q, --quiet  # 程序在运行的过程中不输出信息
-n, --dry-run  # 输出程序会做什么而并不实际执行相关的操作
--version  # 显示版本号
-v, --verbose  # 显示详细的信息
</pre>

```powershell
.
├── build # 编译过程产生的文件
│   ├── CMakeCache.txt
│   ├── CMakeFiles
│   │   ├── ...
│   ├── cmake_install.cmake
│   ├── display_monitor
│   │   ├── ...
│   ├── Makefile
│   ├── proto
│   │   ├── ...
│   ├── rpc_manager
│   │   ├── ...
│   └── test_monitor
│       ├── ...
├── cmake
│   ├── CMakeCache.txt # 核心配置文件，包含所有缓存设置，make过程中的各种变量值
│   ├── CMakeFiles # 构建过程中创建的目录，存储中间构建文件
│   │   ├── ...
│   ├── cmake_install.cmake
│   ├── display_monitor
│   │   ├── CMakeFiles
│   │   │   ├── ...
│   │   ├── cmake_install.cmake
│   │   ├── display
│   │   ├── display_autogen
│   │   │   ├── EWIEGA46WW
│   │   │   │   ├── moc_cpu_load_model.cpp
│   │   │   │   ├── moc_cpu_stat_model.cpp
│   │   │   │   ├── moc_monitor_inter.cpp
│   │   │   │   ├── moc_monitor_model.cpp
│   │   │   │   └── moc_monitor_widget.cpp
│   │   │   ├── moc_predefs.h
│   │   │   └── mocs_compilation.cpp
│   │   └── Makefile
│   ├── Makefile # 描述如何构建项目
│   ├── proto # 根据.proto文件自动生成的文件，会通过编译器集成
│   │   ├── CMakeFiles
│   │   │   ├── CMakeDirectoryInformation.cmake
│   │   │   ├── monitor_proto.dir
│   │   │   │   ├── ...
│   │   │   └── progress.marks
│   │   ├── cmake_install.cmake
│   │   ├── libmonitor_proto.a
│   │   ├── Makefile
│   │   ├── monitor_info.grpc.pb.cc # 实现了下面文件中声明的接口，包含与 gRPC 服务器进行交互的逻辑
│   │   ├── monitor_info.grpc.pb.h # 声明了 gRPC 服务的客户端和服务器接口
│   │   ├── monitor_info.pb.cc # 实现了下面文件中声明的类的成员函数，负责数据的序列化和反序列化等
│   │   └── monitor_info.pb.h # 包含了protobuf消息类的声明，并提供对数据的操作方法
│   ├── rpc_manager
│   │   ├── client
│   │   │   ├── CMakeFiles
│   │   │   │   ├── ...
│   │   │   ├── cmake_install.cmake
│   │   │   ├── libclient.a
│   │   │   └── Makefile
│   │   ├── CMakeFiles
│   │   │   ├── CMakeDirectoryInformation.cmake
│   │   │   └── progress.marks
│   │   ├── cmake_install.cmake
│   │   ├── Makefile
│   │   └── server
│   │       ├── CMakeFiles
│   │       │   ├── ...
│   │       ├── cmake_install.cmake
│   │       ├── Makefile
│   │       └── server
│   └── test_monitor
│       ├── CMakeFiles
│       │   ├── CMakeDirectoryInformation.cmake
│       │   └── progress.marks
│       ├── cmake_install.cmake
│       ├── Makefile
│       └── src
│           ├── CMakeFiles
│           │   ├── ...
│           ├── cmake_install.cmake
│           ├── Makefile
│           └── monitor
├── CMakeLists.txt
├── display_monitor # 监控面板
│   ├── CMakeLists.txt
│   ├── cpu_load_model.cpp
│   ├── cpu_load_model.h # 用于展示来自监控系统的CPU负载数据，并支持数据更新和视图同步
│   ├── cpu_softirq_model.cpp
│   ├── cpu_softirq_model.h # 用于展示来自监控系统的软中断数据，并支持数据更新和视图同步
│   ├── cpu_stat_model.cpp
│   ├── cpu_stat_model.h # 用于展示来自监控系统的CPU使用数据，并支持数据更新和视图同步
│   ├── main.cpp # 基于Qt的GUI应用程序，是一个监控面板，与gRPC服务器通信，获取和显示监控信息
│   ├── mem_model.cpp
│   ├── mem_model.h # 用于展示来自监控系统的内存数据，并支持数据更新和视图同步
│   ├── monitor_inter.cpp
│   ├── monitor_inter.h #提供一个抽象的数据模型类，用于管理表格数据
│   ├── monitor_widget.cpp
│   ├── monitor_widget.h # 包含自定义的监视器小部件类，用于在 GUI 中显示监控数据
│   ├── net_model.cpp
│   └── net_model.h # 用于展示来自监控系统的网络数据，并支持数据更新和视图同步
├── docker
│   ├── build
│   │   ├── apt
│   │   │   └── sources.list
│   │   ├── base.dockerfile # 编译构建容器镜像
│   │   └── install # 一些压缩包和安装脚本
│   │       ├── abseil
│   │       ├── cmake
│   │       ├── grpc
│   │       ├── protobuf
│   │       └── qt
│   └── scripts # 用户用来进入容器的脚本
│       ├── monitor_docker_into.sh # 进入容器
│       └── monitor_docker_run.sh # 启动容器
├── proto  # 包含所有监控信息的.proto文件，定义数据结构和服务接口，为gRPC通信服务
│   ├── CMakeLists.txt
│   ├── cpu_load.proto
│   ├── cpu_softirq.proto
│   ├── cpu_stat.proto
│   ├── mem_info.proto
│   ├── monitor_info.proto
│   └── net_info.proto
├── rpc_manager
│   ├── client
│   │   ├── CMakeLists.txt
│   │   ├── main.cpp # 创建一个RpcClient实例，将monitor_info发送到gRPC服务器
│   │   ├── rpc_client.cpp
│   │   └── rpc_client.h # 自定义的客户端类 RpcClient，用于与远程gRPC服务器通信
│   ├── CMakeLists.txt
│   └── server # 实现gRPC服务器，监听来自客户端的请求并提供服务
│       ├── CMakeLists.txt
│       ├── main.cpp
│       ├── rpc_manager.cpp
│       └── rpc_manager.h # 定义一个gRPC服务实现类
└── test_monitor # 获得监控数据
    ├── CMakeLists.txt
    ├── include
    │   ├── monitor
    │   │   ├── cpu_load_monitor.h # 获取cpu负载数据，下类似
    │   │   ├── cpu_softirq_monitor.h
    │   │   ├── cpu_stat_monitor.h
    │   │   ├── mem_monitor.h
    │   │   ├── monitor_inter.h
    │   │   └── net_monitor.h
    │   └── utils
    │       ├── read_file.h # 读文件
    │       └── utils.h
    └── src
        ├── CMakeLists.txt
        ├── main.cpp # 启动一个线程，定期从不同的监控模块获取系统监控数据，并通过 gRPC 客户端将这些数据发送到服务器
        ├── monitor
        │   ├── cpu_load_monitor.cpp
        │   ├── cpu_softirq_monitor.cpp
        │   ├── cpu_stat_monitor.cpp
        │   ├── mem_monitor.cpp
        │   └── net_monitor.cpp
        └── utils
            └── read_file.cpp 
```

## 构建过程

1. 构建项目需要的环境容器
   
   `base.dockerfile` 中构建了装有所有环境的容器，安装了各种工具如`git`, `vim`,`cmake`,`grpc`等，执行完成后可以在 `docker images` 中看到
   ```sh
   # ./docker/build
   docker build --network host -f base.dockerfile . 
   ```
   
2. 进入容器
   
   启动刚才构建的容器，并且将容器内的 `work` 目录挂载到主机的工作目录，随后以root用户身份，交互式地启动并进入容器
   ```sh
   cd <scripts_content>
   #启动容器
   ./monitor_docker_run.sh 
   #进入容器
   ./monitor_docker_into.sh
   ```
   
3. 在容器中编译项目代码
   ```sh
   # 进入挂载目录
   cd /work
   # 进入编译目录
   cd cmake
   # 根据..的CMakeLists.txt文件生成构建系统
   cmake ..
   # 使用8个并行任务加速构建
   make -j8
   ```
   
4. `./CmakeLists.txt` 
   
   先指定了 `Cmake` 版本，然后设置 `cmake` 模块所在路径和C++编译标准版本，然后依次 `add_subsidirectory` 添加并构建子目录：
   - `add_subdirectory(rpc_manager)`
     
     添加并构建子目录：
     - `add_subdirectory(server)`
       
       将 `server` 编译为可执行文件
     - `add_subdirectory(client)`
       
       将 `client` 编译为静态库
   - `add_subdirectory(test_monitor)`
     
     将 `monitor` 编译为可执行文件
   - `add_subdirectory(proto)`
     
     加载系统中安装的 `protobuf, gRPC, c-ares` 库，将 `monitor_info` 编译为静态库，然后根据 `.proto` 文件生成 `gRPC` 和 C++代码
   - `add_subdirectory(display_monitor)`
     
     加载系统中安装的 `Qt5, Core, Widgets` 库，将 `display_monitor` 编译为可执行文件
   
5. 先运行 `gRPC` 的服务器
   ```sh
   cd /work/build/rpc_manager/server
   ./server
   ```
   
   若不执行这步，而直接 `./monitor` 的话，会显示连接失败
   
6. 启动监控服务
   
   新建一个终端，进入容器
   ```sh
   cd /work/build/test_monitor/src
   ./monitor 
   ```
   
7. 在客户端显示
   
   新建一个终端，进入容器
   ```sh
   cd /work/build/display_monitor
   ./display 
   ```

## CMake

### 简介

- 管理源代码构建的工具，广泛用于C/C++，也可以其他语言，便于**跨平台安装、编译**，简化软件的构建、测试、打包过程。
- 支持多种操作系统，可以为不同的编译环境生成相应的构建文件，如Unix Makefiles、Ninja、Visual Studio

### 配置文件

- 通过编写 `CMakeLists.txt` 配置文件，CMake能够生成标准的构建文件（如Makefile或Visual Studio项目文件），从而简化了项目的构建过程。开发者只需使用`cmake`和`make`等命令即可完成编译和链接等操作
- `CMakeLists.txt` 文件中描述了如何编译和构建项目，cmake构建过程中，该文件会告诉CMake如何查找源代码、依赖库、设置编译选项、生成目标文件、如何组织整个构建过程

### 常用命令

1. 指定 `cmake` 的最小版本
   ```
   // 可选命令，但如果使用了高版本cmake特有的命令时，就需要加这一行
   cmake_minimum_required(VERSION 3.4.1)
   ```
2. 设置项目名称
   ```
   // 非强制，但最好写
   // 会引入两个变量 demo_BINARY_DIR 和 demo_SOURCE_DIR
   // 自动定义两个等价的变量 PROJECT_BINARY_DIR 和 PROJECT_SOURCE_DIR
   project(demo)
   ```
3. 设置编译类型
   ```
   add_executable(demo demo.cpp) # 生成可执行文件
   add_library(common STATIC util.cpp) # 生成静态库
   add_library(common SHARED util.cpp) # 生成动态库或共享库
   ```
   
   `add_library` **默认生成静态库**，通过以上命令生成文件名字
   - 在 Linux 中是：
     - `demo`
     - `libcommon.a`
     - `libcommon.so`
   - 在 Windows 中是：
     - `demo.exe`
     - `common.lib`
     - `common.dll`
4. 指定编译包含的源文件
   - 明确指定包含哪些源文件
     ```
     # demo.cpp相当于demo的main函数文件
     # demo.cpp include "test.h"和"util.h"，所以要把其cpp函数链接进来
     # 一般会先用test.cpp以及其所有用到的所有相关文件先做一个动态库
     add_library(demo demo.cpp test.cpp util.cpp)
     ```
   - 搜索所有的 `cpp` 文件
     ```
     # aux_source_directory(dir VAR) 发现一个目录下所有的源代码文件并将列表存储在一个变量中
     aux_source_directory(. SRC_LIST) # 搜索当前目录下的所有.cpp文件
     add_library(demo ${SRC_LIST})
     ```
   - 自定义搜索规则
     ```
     file(GLOB SRC_LIST "*.cpp" "protocol/*.cpp")
     add_library(demo ${SRC_LIST})
     # 或者
     file(GLOB SRC_LIST "*.cpp")
     file(GLOB SRC_PROTOCOL_LIST "protocol/*.cpp")
     add_library(demo ${SRC_LIST} ${SRC_PROTOCOL_LIST})
     # 或者
     file(GLOB_RECURSE SRC_LIST "*.cpp") #递归搜索
     FILE(GLOB SRC_PROTOCOL RELATIVE "protocol" "*.cpp") # 相对protocol目录下搜索
     add_library(demo ${SRC_LIST} ${SRC_PROTOCOL_LIST})
     # 或者
     aux_source_directory(. SRC_LIST)
     aux_source_directory(protocol SRC_PROTOCOL_LIST)
     add_library(demo ${SRC_LIST} ${SRC_PROTOCOL_LIST})
     ```
5. 查找指定的库文件
   
   `find_library(VAR name path)`查找到指定的预编译库，并将它的路径存储在变量中。
   默认的搜索路径为 `cmake` 包含的系统库。类似的命令还有 `find_file()`、`find_path()``、`find_program()`、`find_package()`
   ```
   find_library( # Sets the name of the path variable.
                 log-lib
    
                 # Specifies the name of the NDK library that
                 # you want CMake to locate.
                 log )
   ```
6. 设置包含的目录
   
   将指定目录添加到编译器的头文件搜索路径之下
   ```
   include_directories(
       ${CMAKE_CURRENT_SOURCE_DIR}
       ${CMAKE_CURRENT_BINARY_DIR}
       ${CMAKE_CURRENT_SOURCE_DIR}/include
   )
   ```
7. 设置链接库搜索目录
   ```
   link_directories(
       ${CMAKE_CURRENT_SOURCE_DIR}/libs
   )
   ```
8. 指定目标需要包含的头文件路径
   - 需要用关键字来指定参数的范围
   ```
   target_include_directories(monitor PUBLIC
     ${PROJECT_SOURCE_DIR}/test_monitor/include
     ${PROJECT_SOURCE_DIR}/rpc_manager
   )
   ```
9. 设置 target 需要链接的库
   
   用于指定一个目标（如可执行文件或库）依赖于哪些其他目标。
   
   此处可以使用关键字来定义链接关系的作用范围：如 ` target_link_libraries(client PUBLIC monitor_proto)`
   - `PUBLIC`：链接到 `client` 的库会传播到依赖 `client` 的其他目标。也就是说，如果有其他目标链接到 `client`，那么这些目标也会自动链接 `monitor_proto`
   - `PRIVATE`：只有 `client` 目标会链接这个库，依赖 `client` 的其他目标不会继承这个链接关系
   - `INTERFACE`：这个库不会直接链接到 `client`，而是通过 `client` 传递给其他依赖 client 的目标
   ```
   target_link_libraries( # 目标库
                          demo
    
                          # 目标库需要链接的库
                          # log-lib 是上面 find_library 指定的变量名
                          ${log-lib} )
   ```
   - 指定链接动态库或静态库
     ```
     target_link_libraries(demo libface.a) # 链接libface.a
     target_link_libraries(demo libface.so) # 链接libface.so
     ```
   - 指定全路径
     ```
     target_link_libraries(demo ${CMAKE_CURRENT_SOURCE_DIR}/libs/libface.a)
     target_link_libraries(demo ${CMAKE_CURRENT_SOURCE_DIR}/libs/libface.so)
     ```
   - 指定链接多个库
     ```
     target_link_libraries(demo
         ${CMAKE_CURRENT_SOURCE_DIR}/libs/libface.a
         boost_system.a
         boost_thread
         pthread)
     ```
10. 设置变量
    - set 直接设置变量的值
      ```
      set(SRC_LIST main.cpp test.cpp)
      add_executable(demo ${SRC_LIST})
      ```
    - set 追加设置变量的值
      ```
      set(SRC_LIST main.cpp)
      set(SRC_LIST ${SRC_LIST} test.cpp)
      add_executable(demo ${SRC_LIST})
      ```
    - list 追加或者删除变量的值
      ```
      set(SRC_LIST main.cpp)
      list(APPEND SRC_LIST test.cpp)
      list(REMOVE_ITEM SRC_LIST main.cpp)
      add_executable(demo ${SRC_LIST})
      ```
11. 条件控制
12. 添加子目录并构建该子目录
    - 指定一个子目录 `sub_dir`，子目录下应该包含 `CMakeLists.txt` 文件和代码文件
    - `binary_dir` 为可选参数，指定一个目录，用于存放输出文件
    ```
    add_subdirectory(sub_dir [binary_dir])
    ```
13. 打印信息
    ```
    message(${PROJECT_SOURCE_DIR})
    message("build with debug mode")
    message(WARNING "this is warnning message")
    message(FATAL_ERROR "this build has many error") # FATAL_ERROR 会导致编译失败
    ```
14. 包含其他 `cmake` 文件
    ```
    include(./common.cmake) # 指定包含文件的全路径
    include(def) # 在搜索路径中搜索def.cmake文件
    set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake) # 设置include的搜索路径
    ```

## protobuf

### 简介

- 全称为Protocol Buffers，是Google开发的一种轻量级的数据交换格式，可以理解为一种**更灵活、高效的数据格式**
- 能够序列化（数据结构/对象->二进制串）和反序列化（二进制串->数据结构/对象），属于通信协议的一部分，在数据结构和对象 与 用于存储和传输的格式 之间相互传输
- 在网络传输和数据存储等场景中广泛应用，特别适用于**对时间效率或空间效率方面有极高要求**的场景，如服务器间的海量数据传输与通信。通过使用Protobuf，可以简化数据的存储和传输过程，提高系统的性能和可靠性

### 为什么需要（反）序列化

- 数据可持久化：
  - 序列化：通过将内存中的对象或数据结构转换为可存储的结构，如：二进制、json、xml等，数据就可以被保存在文件系统、数据库或其他持久存储介质中
  - 反序列化：从存储介质中读取序列化的数据，并还原为内存中的对象或数据结构，允许数据在应用程序关闭后得以保留和恢复

- 网络通信：
  - 发送方将数据序列化为可传输的格式，数据能够在不同的计算机之间通过网络传递。接收方通过反序列化将接收到的数据还原成原始对象和数据结构，以便在本地使用。

- 跨语言：

### 序列化协议特性

每种序列化协议都有优缺点，系统设计过程中需根据特性选择合适的

- 通用性：是否与语言、平台无关
- 鲁棒性
- 可调试性/可读性：protobuf不可读，XML、JSON可读
- 性能：因为使用二进制格式，相比于其他序列化机制，如XML和JSON，具有更高的性能和更小的数据体积
- 可扩展性/兼容性：protobuf支持数据结构的向前和向后兼容。当数据结构发生变化时，可以向旧的数据结构中添加新的字段，而不会影响已有的数据，可扩展性强
- 安全性/访问限制：往往考虑跨局域网访问的场景

**为什么pb会提高传输效率？**

- XML、JSON在进行数据编译时，数据文本格式更容易阅读，但进行数据交换时，设备就需要耗费大量的CPU在I/O动作上，会影响整个传输速率
- 而pb会将字符串序列化后（即**二进制数据**）再进行传输，字节数会比JSON、XML少很多，速率更快
- 序列化数据非常简洁、紧凑，与XML相比，序列化之后的数据量约为1/3-1/10
- 解析速度非常快，比对应的XML快约20-100倍

### 序列化底层组件

- IDL（Interface description language）文件：参与通讯的各方需要对通讯的内容做相关的约定，为建立一个与语言和平台无关的约定，约定需要采用与具体开发语言、平台无关的语言来进行描述，这种语言被称为接口描述语言（IDL），采用IDL撰写的协议约定称之为IDL文件。
- IDL Compiler：编译器，将IDL文件转换成各语言对应的动态库。
- Stub/Skeleton Lib：负责序列化和反序列化的工作代码。Stub是一段部署在分布式系统客户端的代码，一方面接收应用层的参数，并对其序列化后通过底层协议栈发送到服务端，另一方面接收服务端序列化后的结果数据，反序列化后交给客户端应用层；Skeleton部署在服务端，其功能与Stub相反，从传输层接收序列化参数，反序列化后交给服务端应用层，并将应用层的执行结果序列化后最终传送给客户端Stub。
- Client/Server：指的是应用层程序代码，他们面对的是IDL所生存的特定语言的class或struct。
- 底层协议栈和互联网：序列化之后的数据通过底层的传输层、网络层、链路层以及物理层协议转换成数字信号在互联网中传递。

### proto文件最终生成了什么

当编译 `xxx.proto` 时，编译器会以您选择的语言生成代码，您需要使用文件中描述的消息类型，包括获取和设置字段值、将消息序列化为输出流，并从输入流解析消息。

- 对于C++，编译器会根据每个 `.proto` 生成一个 `.h` 和 `.cc` 文件，其中包含文件中描述的每种消息类型的类。
  - `protoc` 处理 `.proto` 文件时，会为 message 定义数据结构成类，并且为其中的每个字段生成 ：
    - Getter（获取值）
    - Setter（设置值）
    - Mutable 方法（获取可变引用）
    - 内部访问方法
    
    这些函数名字都是比较类似、固定的，所以看开发文档就可以知道调用什么
- 对于Java，编译器会生成一个 `.java` 文件，其中包含每个消息类型类，以及 `Builder` ，用于创建消息类实例的特殊类。

```powershell
protoc --proto_path=IMPORT_PATH --cpp_out=DST_DIR --java_out=DST_DIR  path/to/file.proto
```

### ProtoBuf使用步骤

1. Step1：定义 proto 文件，文件的内容就是定义我们需要存储或者传输的数据结构，也就是定义我们自己的数据存储或者传输的协议
   ```protobuf
   // 定义 test.proto 文件
   syntax = "proto3";
   package monitor.proto;
   
   // message关键字用于定义消息类型，允许嵌套
   message CpuLoad {
       float load_avg_1 = 1;
       float load_avg_3 = 2;
       float load_avg_15 = 3;
     }
     
   message NetInfo {
       string name = 1;
       float send_rate = 2;
     }
   
   // service关键字用于定义gRPC服务接口
   // 用于声明一个服务，服务包含多个 RPC 方法，每个方法对应于客户端和服务器之间的一个远程调用
   service GrpcManager {
     rpc SetMonitorInfo(MonitorInfo) returns (google.protobuf.Empty) {
     }
   
     rpc GetMonitorInfo(google.protobuf.Empty) returns (MonitorInfo) {
     }
   }
   ```
2. Step2：使用编译器 protoc 来编译自定义的 proto 文件，用于生成 .pb.h 文件（proto 文件中自定义类的头文件）和 .pb.cc（proto文件中自定义类的实现文件）
   ```powershell
   # 编译
   protoc -I$SRC_DIR --cpp_out=$DST_DIR test.proto
   # 参数解释
   # -I$SRC_DIR 指定test.proto所在目录
   # --cpp_out=$DST_DIR 指定生成cpp相关文件，并指定生成路径
   # test.proto 需要编译的proto文件
   ```
3. 使用 ProtoBuf 的 C++ API 来读写消息

## proc

### 简介

在 Linux 中，`/proc` 是位于内存中的 **虚拟文件系统**，其中保存**运行时信息**，比如系统内存、磁盘IO、设备挂载信息、硬件配置等，所以proc是一个控制中心，用户可以通过更改其中某些文件来改变内核的运行状态；也是查询中心，可以通过这些文件查看有关系统硬件及当前正在运行进程的信息。在Linux系统中，许多工具的数据来源正是proc目录中的内容。相当于是内核空间和用户空间之间的通信窗口。

### 使用和指标

- `/proc/stat` 提供系统的CPU和任务统计信息，`top` 命令就是通过其中数据进行换算得出
- `/proc/loadavg` 保存了系统负载的平均值，其前三列分别表示最近1分钟、5分钟及15分的平均负载，平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。CPU使用率是指在单位时间内CPU处在非空闲态的时间比，反映了CPU的繁忙程度。平均负载最理想的情况是等于 CPU 个数。
- `/proc/softirqs` 记录自开机以来软中断累积次数
- `/proc/interrupts` 记录自开机以来的累积中断次数
- `/proc/meminfo` 当前内存使用的统计信息
- `/proc/net/dev` 网络流入流出的统计信息，包括接收包的数量、发送包的数量，发送数据包时的错误和冲突情况等

## stress

### 简介

Linux `stress` 命令主要用来模拟系统负载较高时的场景

### 语法

```powershell
stress <options>
# 参数
-c, --cpu N  # 产生 N 个进程，每个进程都反复不停的计算随机数的平方根
-i, --io N  # 产生 N 个进程，每个进程反复调用 sync() 将内存上的内容写到硬盘上
-m, --vm N  # 产生 N 个进程，每个进程不断分配和释放内存
    --vm-bytes B  # 指定分配内存的大小
    --vm-stride B  # 不断的给部分内存赋值，让 COW(Copy On Write)发生
    --vm-hang N  # 指示每个消耗内存的进程在分配到内存后转入睡眠状态 N 秒，然后释放内存，一直重复执行这个过程
    --vm-keep  # 一直占用内存，区别于不断的释放和重新分配(默认是不断释放并重新分配内存)
-d, --hadd N  # 产生 N 个不断执行 write 和 unlink 函数的进程(创建文件，写入内容，删除文件)
    --hadd-bytes B  # 指定文件大小
-t, --timeout N  # 在 N 秒后结束程序        
--backoff N  # 等待N微妙后开始运行
-q, --quiet  # 程序在运行的过程中不输出信息
-n, --dry-run  # 输出程序会做什么而并不实际执行相关的操作
--version  # 显示版本号
-v, --verbose  # 显示详细的信息
```

## gRPC

### 简介

- 一个现代的、高性能的开源远程过程调用（RPC）框架，它可以在任何环境中运行，并高效地连接数据中心内和跨数据中心的服务
- gRPC的高性能和多语言支持使其成为微服务架构中的理想选择，适用于实时数据流的传输，如聊天应用、视频流、股票市场数据等，还有物联网场景

### 特点

- 基于**HTTP2.0**协议，支持双向流、头部压缩和多路复用，这些特性使得gRPC具有**更低的网络延迟和更高的吞吐量**
- 支持多种编程语言，如C、C++、Java、Go、Python、Ruby等，这使得它能够在**多语言协作**的分布式系统中发挥重要作用
- 序列化支持PB和JSON，**默认使用PB**，提供自动生成客户端和服务器代码的工具，大大简化开发流程，提高开发效率
- 安装简单、扩展方便（用该框架每秒可达到百万个RPC）

### 交互过程

- 在gRPC中，客户端应用程序可以直接调用不同机器上服务器应用程序的方法，就像调用本地对象一样，它基于定义服务的思想，指定可以远程调用的方法、参数、返回类型；**在服务器端**，服务器实现此接口并运行gRPC服务器来处理客户端调用；**在客户端**，客户端有一个stub存根，提供与服务器相同的方法

  ![1741869841983](D:\WeChat\WeChat Files\wxid_4njvjhupxgbc22\FileStorage\Temp\1741869841983.png)

  

- 若**使用Protocol Buffers（protobuf）作为接口定义语言**，定义严格的接口和数据结构，这有助于避免数据格式不一致的问题。gRPC 使用带特殊 gRPC 插件的 `protoc` 从 `.proto` 文件生成代码，从而可以获得生成的 gRPC 客户端和服务器代码，以及用于填充、序列化和检索消息类型的常规 pb 代码。比如下图中的2-11步全都封装起来，对用户透明

  ![1741872071247](D:\WeChat\WeChat Files\wxid_4njvjhupxgbc22\FileStorage\Temp\1741872071247.png)

  ![image-20250313213022181](C:\Users\GotGyu\AppData\Roaming\Typora\typora-user-images\image-20250313213022181.png)

### 和http协议比较

![2041a9ae2025b9f96a70ff0a04c3dad.png](6571cc310d34578820d7ee8db407a087.png)

### 高性能

介绍如何从gRPC、利用http2.0特性获得最佳性能

1. **重用 gRPC 通道**

   进行gRPC调用时，应重用通道，可以在现有的HTTP2.0连接上对调用进行多路复用，不然如果为每个gRPC调用创建一个新通道的话，每次都需要通过以下步骤创建新的HTTP2.0连接：

   - 打开套接字
   - 建立 TCP 连接
   - 协商 TLS
   - 启动 HTTP/2 连接
   - 进行 gRPC 调用

2. **连接并发**

   http2.0连接通常会限制一个连接上同时存在的最大并发流

   - 可以为具有高负载的应用的区域创建单独的gRPC通道
   - 可以使用gRPC通道池，例如创建gRPC通道列表

3. **客户端应用中的异步调用**

   - 首选调用gRPC方法时将异步编程与async,await配合使用，使用阻塞（如 `Task.Result` 或 `Task.wait()`）进行 gRPC 调用会使其他任务无法使用线程，这可能会导致线程池资源不足、性能不佳、应用程序因死锁而挂起
   - 所有 gRPC 方法类型都在 gRPC 客户端上生成异步 API

4. **负载均衡**

   - 只有两种方法，其他的负载均衡器不能与gRPC一起高效工作：

     - **客户端负载均衡**

       （没看懂）

     - **L7（应用程序）代理负载均衡**

       L7代理在一个HTTP2.0连接上接收多路复用的gRPC调用，并将它们分发到多个后端终结点上，比上一个方案简单，但是可能效率没那么高

5. **保持活动 ping**

   - 可用于在非活动期间使HTTP2.0连接保持为活动状态，如果在应用恢复活动时已准备好现有 HTTP2连接，则可以快速进行初始gRPC调用，不会因重新建立连接而导致延迟

# rust量化交易系统

## 设计思路

### 目录架构

```powershell
.
├── LICENSE
├── README.md
└── xcrypto
    ├── binance
    │   ├── Cargo.toml
    │   ├── spot
    │   │   ├── Cargo.toml
    │   │   └── src
    │   │   │   └── main.rs         # 主函数，读取配置文件，启动主线程持续接受市场数据和执行交易策略
    │   │   │   └── trade.rs        # 实现了一个spottrade交易逻辑，包含ws监听、订单处理、会话管理等功能
    │   ├── src
    │   │   ├── account.rs          # 实现一个ws客户端，用于与某个ws服务进行连接
    │   │   ├── app.rs              # 实现了一个 WebSocket 服务器，用于管理 WebSocket 连接，并与 Trade 交易逻辑进行交互
    │   │   ├── chat.rs             # Binance 交易所的 WebSocket 数据解析，通过 serde 序列化和反序列化结构体，来处理实时市场数据
    │   │   ├── handler.rs          # 实现了一个 ws 服务器的 Handler（处理器），是 ws server的核心，用于管理客户端连接、处理 WebSocket 消息，并与交易系统进行交互
    │   │   ├── lib.rs              # 定义了一个交易相关的 Rust 模块系统，并声明了一些核心的 trait（特性）用于管理市场数据、订单处理和交易行为
    │   │   ├── market.rs           # 订阅管理模块，用于处理客户端连接、订阅、取消订阅和市场数据流的转发
    │   │   ├── session.rs          # 管理用户的交易会话，包括用户的持仓信息、订单处理和消息发送
    │   │   └── subscriber.rs       # 管理用户的订阅关系，维护订阅的交易对，并处理 WebSocket 消息转发
    │   └── usdt
    │   │   ├── Cargo.toml
    │   │   └── src
    │   │   │   └── main.rs         # 和./spot差不多，另一种市场
    │   │   │   └── trade.rs
    ├── Cargo.lock
    ├── Cargo.toml
    ├── logger                      # 日志模块
    │   ├── Cargo.toml
    │   └── src
    │       └── lib.rs
    ├── pyalgo                      # python 策略模块
    │   ├── Cargo.toml
    │   ├── pyproject.toml
    │   ├── python                  # python写的策略
    │   │   └── pyalgo
    │   └── src                     # rust 提供的接口
    │       ├── chat.rs
    │       ├── constant.rs
    │       ├── lib.rs              # Rust + PyO3 绑定模块，用于提供 Python 接口，使 Python 代码可以调用 Rust 代码的功能
    │       ├── phase.rs
    │       ├── rest.rs
    │       ├── session.rs
    │       ├── subscription.rs
    │       └── ws.rs                 
    ├── src
    │   ├── chat.rs                 # 定义了一套用于金融交易的数据结构
    │   ├── error.rs                # 定义了几个error id
    │   ├── lib.rs                  # 导入        
    │   ├── parser.rs               # 封装了 serde_json 的解析功能，提供了 Parser 结构体用于管理 JSON 数据
    │   ├── position.rs             # 封装了 Position 数据的数据库操作，提供数据存取、异步操作
    │   ├── rest.rs                 # REST API客户端，用于发送http请求到binance API，并提供基础的下单/撤单
    │   └── ws.rs                   # 实现了一个基于 WebSocket 协议的通信模块，支持 客户端 和 服务端 角色
    └── target
        ├── CACHEDIR.TAG
        └── debug
```

### 模块

- 数据获取模块
  - 用 `reqwest` 定期请求API数据，解析API返回的JSON数据，提取所需字段，存到Redis或SQLite
- websocket服务模块：实时推送市场数据到客户端
  - 用 `tokio-tungstenite` 实现ws服务器
  - 支持多客户端连接，每个客户端独立订阅不同的数据
- 策略执行模块：根据市场数据执行交易策略
  - 得到市场数据，根据策略逻辑生成交易信号（比如买入或卖出），传给下一层
  - 使用 `tokio::spawn` 并行执行多策略
- 订单管理模块：处理订单生成、发送、状态更新 
  - 生成订单，监听订单状态，更新本地记录
- 会话管理模块：管理客户端连接、会话状态
  - 为每个客户端分配唯一的会话ID，记录客户端的订阅消息，在断开连接后清理会话状态
- 风控模块

![1742046300013](D:\WeChat\WeChat Files\wxid_4njvjhupxgbc22\FileStorage\Temp\1742046300013.png)

## 开发流程

1. 市场数据处理（行情网关）
   - 使用 ashares 库获取行情数据（K 线、逐笔成交、盘口等）
   - 解析数据并存储到合适的数据结构（如 struct Kline {}）
   - 提供数据存取接口（如 get_kline()）
   - 选择数据存储方式（什么数据库）
   - 设计数据库表结构，存储历史行情数据（需要吗）
   - 架构：
     - 数据获取层
     - 数据解析层
     - 数据存储层
     - 数据接口层
2. 交易系统架构
   - 账户管理
     - 维护账户资金、持仓、交易记录
     - 设计 struct Account {} 结构体，包含可用资金、持仓等信息
   - 订单管理（订单管理模块）
     - 设计 struct Order {}，包括订单类型、状态、价格、数量等
     - 订单撮合逻辑（可选择模拟交易所撮合或使用实盘 API）
     - 订单生命周期管理（Pending → Executed → Completed）
3. 交易策略开发（策略引擎）
   - 策略引擎
     - 设计策略接口 trait Strategy，允许不同策略实现
     - 示例策略：均线策略、动量策略、回归策略
   - 回测框架
     - 读取历史数据，按时间推进，模拟执行订单
     - 计算策略的收益率、最大回撤、Sharpe 比率等指标
   - 风控（风控子系统）
     - 设计风控机制，如最大亏损限制、仓位控制
4. 交易执行（交易网关、交易核心、算法交易模块）
   - 交易API
     - 连接券商或交易所 API（如 ashares 提供的交易接口）
     - 提供下单、撤单、查询订单等功能
   - WebSocket 订阅
     - 订阅实时行情数据
     - 处理市场事件，并通知策略模块

## 依赖库

### anyhow

- 用于高效且灵活的错误处理，简化错误传播

### base64

- 正确、快速、可配置的 base64 解码和编码
- 在只允许使用纯文本的情况下，Base64 可以高效地传输二进制数据

### binance

- 使用 Binance API 相关的 Rust 库，包含市场数据、交易功能等

### chrono

- 提供所有关于日期和时间的操作

  DateTime 类型默认具有时区感知功能，并具有单独的无时区感知类型。
可能产生无效或模糊日期和时间的操作会返回 Option 或 MappedLocalTime。
使用 strftime 启发的日期和时间格式化语法，可配置解析和格式化。
本地时区与操作系统的当前时区一致。
类型和操作的实现具有合理的效率。
为了限制二进制文件的大小，时区数据默认不随 chrono 一起提供。请使用配套的 Chrono-TZ 或 tzfile 获取完整的时区支持。

### clap

- 简单易用、高效且功能齐全的命令行参数解析器

### criterion

- 统计驱动的微基准测试库，为检测和估算性能改进和回归的大小提供强大的统计置信度

### crossbeam-channel

- 用于消息传递的**多生产者多消费者**通道
- 该板块可替代 `std::sync::mpsc`，具有更多功能和更好的性能
- 非常适合用于**多线程环境**下的消息传递            

### ctp2rs

- 也许未来可以用
- 对接CTP（期货）官方API，适用于期货、期权市场
- 依赖 `ctpx` C++ 绑定，目前Rust生态支持较少，需要自己维护

### log

- 轻量级日志界面

### maybe-async

- 在一个 crate 中实现同步异步版本时，大多数接口的区别仅在于 `async` `await` 关键字

### native-json

- 为 Rust 提供原生 JSON 语法，将 JSON 语法分析为原生 Rust 结构
- 可以像在 JavaScript 中一样本地声明 JSON 对象。

### reqwest*重点

- 功能丰富、易用性强的HTTP客户端库
- 异步网络库，可以提高并发性能
- 提供了：
  - 支持异步和阻塞式API
  - 灵活的请求体处理(纯文本、JSON、urlencoded、multipart)
  - 可自定义的重定向策略
  - HTTP代理支持
  - 基于系统原生TLS的HTTPS(可选使用rustls)
  - Cookie存储
  - WebAssembly(WASM)支持

### serde-json

- 通常与 `serde` 一起使用，支持 `no_std`

### sqlx

- Rust SQL 工具包，支持异步，纯Rust编写
- 支持 PostgreSQL、MySQL 和 SQLite

### tokio

- tokio 是一个异步运行时，适合高性能网络编程
- **如何利用 tokio 实现高性能网络通信：**
  - 异步编程：`async/await` 语法实现非阻塞操作，避免了线程切换的开销
  - 并发处理：使用tokio 的任务调度器，处理大量并发连接
  - 性能优化：减少锁竞争、优化内存分配、合理配置tokio的线程池

### tokio-tungstenite*重要

- 一个基于 Rust 的异步 WebSocket 库，为 Tokio 提供了非阻塞/异步的 TCP 流绑定和包装
- 支持 TLS（一种密码通信框架），可以通过特性标志启用，适用于需要安全 WebSocket (wss://) 支持的场景

### url

基于 URL 标准的 Rust URL 库

### ws

适用于 Rust 的轻量级事件驱动 WebSockets

## WebSocket

### 简介

- 基于 TCP 的一种新的应用层网络协议
- 提供一个全双工的通道，允许服务器和客户端之间实时双向通信。因此，浏览器和服务器只需要完成**一次握手**，两者之间就直接可以创建持久性的连接，并进行双向数据传输

### 特点

- 建立在 TCP 协议之上，服务器端的实现比较容易
- 与 HTTP 协议有着良好的兼容性，默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器
- 数据格式比较轻量，性能开销小，通信高效
- 可以发送文本，也可以发送二进制数据
- 没有同源限制，客户端可以与任意服务器通信
- 协议标识符是ws（如果加密，则为wss），服务器网址就是 URL

### 和HTTP协议的对比

- HTTP 协议有一个缺陷：通信只能由客户端发起，不具备服务器推送能力。这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。我们只能使用’轮询’：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。轮询的效率低，非常浪费资源
- http协议本身是没有持久通信能力的，但是我们在实际的应用中，是很需要这种能力的，所以，为了解决这些问题，WebSocket协议由此而生
- 在HTML5标准中增加了有关WebSocket协议的相关api，所以只要实现了HTML5标准的客户端，就可以与支持WebSocket协议的服务器进行全双工的持久通信

|性质|websocket|http|
|--|--|--|
|实时性能|更高，允许服务器和客户端之间实时双向通信||
|网络开销|更少，在同一个连接上双向通信|请求和响应之间需要额外的数据传输|
|通信方式|更加灵活，可以以多种方式进行通信，如消息Push、事件推送等|请求和响应一一对应|
|API|API更简洁||
|无连接|不支持无连接，连接不会在一次请求之后立即断开，消除了建立连接的开销，但是也可能导致一些资源泄漏的问题||
|应用范围|一些旧的浏览器可能不支持，且WebSocket 需要服务端支持，只有特定的服务器才能够实现 WebSocket 协议。这可能会增加系统的复杂性和部署的难度||
|兼容性|数据流格式与 HTTP 不同||

### 工作流程

- 通过已建立的TCP连接来传输数据（和http一样），体实现上是通过http协议建立通道，然后在此基础上用真正的WebSocket协议进行通信
- 握手阶段：
  - 客户端向服务端发送请求，请求建立WebSocket连接。请求中包含一个Sec-WebSocket-Key参数，用于生成WebSocket的随机密钥
  - 服务端接收到请求后，生成一个随机密钥，并使用随机密钥生成一个新的Sec-WebSocket-Accept参数
  - 客户端接收到服务端发送的新的Sec-WebSocket-Accept参数后，使用原来的随机密钥和新的Sec-WebSocket-Accept参数共同生成一个新的Sec-WebSocket-Key参数，用于加密数据传输
  - 客户端将新的Sec-WebSocket-Key参数发送给服务端，服务端接收到后，使用该参数加密数据传输
- 数据传输阶段：
  - 建立连接后，客户端和服务端就可以通过WebSocket进行实时双向通信
  - 客户端向服务端发送数据，服务端收到数据后将其转发给其他客户端
  - 服务端向客户端发送数据，客户端收到数据后进行处理
  - 过程中 WebSocket 的每条消息可能会被切分成多个数据帧（最小单位）。发送端会将消息切割成多个帧发送给接收端，接收端接收消息帧，并将关联的帧重新组装成完整的消息
  - 发送方 -> 接收方：ping
  - 接收方 -> 发送方：pong
  - ping 、pong 的操作，对应的是 WebSocket 的两个控制帧
- 关闭阶段： 
  - 当不再需要WebSocket连接时，可以关闭
  - 客户端向服务端发送关闭请求，请求中包含一个WebSocket的随机密钥
  - 服务端接收到关闭请求后，向客户端发送关闭响应，关闭响应中包含服务端生成的随机密钥
  - 客户端收到关闭响应后，关闭WebSocket连接
- 数据帧和控制帧
  - 数据帧
    - 主要包括两个部分：帧头和有效载荷
    - 帧头包括四个部分：fin、rsv1、rsv2、rsv3、opcode、masked 和 payload_length。其中，fin 表示数据帧的结束标志，rsv1、rsv2、rsv3 表示保留字段，opcode 表示数据帧的类型，masked 表示是否进行掩码处理，payload_length 表示有效载荷的长度
    - 有效载荷是数据帧中实际的数据部分，它由客户端和服务端进行数据传输
  - 控制帧
    - Ping 帧：Ping 帧用于测试客户端和服务端之间的连接状态，客户端向服务端发送 Ping 帧，服务端收到后需要向客户端发送 Pong 帧进行响应。
    - Pong 帧：Pong 帧用于响应客户端的 Ping 帧，它用于测试客户端和服务端之间的连接状态。
    - Close 帧：Close 帧用于关闭客户端和服务端之间的连接，它包括四个部分：fin、rsv1、rsv2、rsv3、opcode、masked 和 payload_length。其中，opcode 的值为 8，表示 Close 帧。

### 应用场景

- **即时**聊天通信
- 多玩家游戏
- 在线协同编辑/编辑
- **实时**数据流的拉取与推送
- 体育/游戏实况
- **实时**地图位置
- 即时Web应用程序：即时Web应用程序使用一个Web套接字在客户端显示数据，这些数据由后端服务器连续发送。在WebSocket中，数据被连续推送/传输到已经打开的同一连接中，这就是为什么WebSocket更快并提高了应用程序性能的原因。例如在交易网站或比特币交易中，这是最不稳定的事情，它用于显示价格波动，数据被后端服务器使用Web套接字通道连续推送到客户端
- 游戏应用程序：在游戏应用程序中，你可能会注意到，服务器会持续接收数据，而不会刷新用户界面。屏幕上的用户界面会自动刷新，而且不需要建立新的连接，因此在WebSocket游戏应用程序中非常有帮助
- 聊天应用程序：聊天应用程序仅使用WebSocket建立一次连接，便能在订阅户之间交换，发布和广播消息。它重复使用相同的WebSocket连接，用于发送和接收消息以及一对一的消息传输
- 不能使用WebSocket的场景：如果要获取旧数据，或者只想获取一次数据供应用程序使用，则应该使用HTTP协议，不需要很频繁或仅获取一次的数据可以通过简单的HTTP请求查询

## REST

客户端和服务端通信的一组规则，例如客户端向服务端请求访问指定数据、或在服务端保存数据服务端响应客户端请求过程的过程

### 限制条件

- REST是**无状态**的：通讯过程中，服务端不会保存客户端的上下文信息，即每个请求都需要携带必要的数据



# vKernel

## 简介

一个 Linux 平台下用于容器的可定制、可插拔的轻量级虚拟内核，是一种兼顾容器安全和性能的虚拟化技术。终极目标是达到VM级别的安全，短期目标是规避掉常见的runc安全漏洞（典型容器安全漏洞如：容器逃逸，覆盖runc二进制；文件挂载；提权为root）

![image-20250319205528276](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250319205528276.png)

基于内核可加载模块技术在内核层实现虚拟内核，旨在打破容器场景下的内核共享的局限性，实现容器独立的虚拟内核。

vkernel 包含如下几个部分：

- **module**：vkernel 内核模块（vkernel.ko）。它是 vkernel 的核心部分，为容器实现内核资源的虚拟化及安全防护。
- **builder**：vkernel 内核模块构建工具，用于分析容器镜像系统调用，基于 seccomp、apparmor 规则自动化构建 vkernel 内核模 块。
- **runtime**：一个调用 vkernel 内核模块的容器运行时。运行时兼容 [OCI](https://github.com/opencontainers/runtime-spec)  标准，基于 [runc 1.0.0-rc92](https://github.com/opencontainers/runc/tree/v1.0.0-rc92)。
- **kernel**：运行 vkernel 内核模块的 Linux 内核，基于 [Linux 5.7](https://github.com/torvalds/linux/tree/v5.7)。

![image-20250321151721359](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250321151721359.png)

## 特性

### vkernel 实现的特性

- **虚拟内核安全隔离**
  
  - **系统调用隔离**：为容器创建独立的系统调用表，以及在 vkernel 内核模块中实现 futex 子系统。
  - **文件访存控制**：基于 inode 虚拟化实现文件及目录的自定义访问规则。
  - **数据隔离**：（本功能需要修改内核，其他功能可以通过hotfix, inline hook等内核无侵入的方式实现）
    - 内核数据隔离：隔离内核权限数据、内容数据，隔离容器的内核日志访问、保护权限控制数据防止越权逃逸
    - 用户数据隔离：隔离匿名映射、管道、RAMFS
  
  - **内核模块地址空间隔离**：为每个 vkernel 模块生成独立的地址空间。
  
- **虚拟内核功能定制**

  - 内核参数隔离：独立的内核参数控制（`/sys/`, `/proc/sys` 等路径下）
    - 内核资源用量限制：限制fd/inode等抽象内核资源用量
    - 宏观资源策略并存：对CPU/内存等资源的调度或使用策略，容器可以各异【我具体负责了这一块，内容在下一节】
    - 微观资源参数定制：容器可经性能调优后，独立设置合适的sysctl参数


### 特性带来的优势

- **安全性**：相比普通容器、apparmor、seccomp，更轻量的安全机制，也就是系统调用隔离、文件访存隔离开销更低
- **运行性能**：支持独立的资源视图、资源用量限制以及**可定制**的资源策略，能更好地利用计算资源提供更优的性能独立
  - 资源视图：辅助应用进行资源配置，如并发度、堆大小
  - 资源策略定制：以适合应用的方式调度资源

- **资源占用**：没有额外资源占用，cpu与内存的占用与普通容器相当
- 具体而言↓
- **系统调用隔离**
  - 更加灵活的系统调用参数过滤。可用作seccomp的替代，可以有更多样的表现。如解引用指针参数，拦截非法调用同时记录日志，发现高频非法调用时告警，不像seccomp或内核补丁那样受限或影响整机
  - 快速修复内核漏洞。有些内核漏洞是缺乏边界检查造成的，简单修改容器系统调用表即可修复，无需更换内核。
  - 特定调用加速。可专门为目标容器编写新的系统调用实现，以加速其高频系统调用。如分配专门的缓存，加速一些只读文件元数据的访问。
  - 系统调用调试。可非常容易的新增系统调用，进行功能调试，无需重编译内核。

- 文件访问控制
  - 更加高效的文件访问控制。可用作apparmor的替代，基于inode标记检索文件，更加高效可靠。针对频繁的非法访问，可以有更多样的自定义处理手段。

- 内核日志隔离
  - 避免泄露敏感日志信息。主机或其他容器产生的内核日志可能包含敏感信息，不应泄露。

- 内核参数隔离
  - 避免内核资源被恶意占用。如限制容器最大文件数，避免恶意容器过多占用fd。
  - 冲突负载并存。如redis推荐overcommit_memory=1，而postgresql推荐设置为2，可分别独立配置。
  - 特定负载性能调优。如mysql受益于透明大页访存，可单独设置thp enable=always，而主机保持madvise或never减少内存浪费。

## 架构设计思想

### 总体设计

安全容器基于硬件虚拟化并为容器提供独立内核，vkernel框架基于原生容器的特点（消除硬件虚拟化层、共享宿主机内核），为容器提供一个额外的虚拟内核，在虚拟内核中私有化容器依赖的最小内核代码与数据。

同时，设计的时候考虑到了：

1. 轻量高效：vkernel框架必须以最小的性能开销来实现容器与宿主内核的解耦，不影响容器应用的正常运行
   - 做法：消除硬件虚拟化层，为容器提供一个包含私有内核代码与数据的vkernel实例，解除容器对部分内核机制的依赖（具体做法与gVisor类似，但它性能低）
2. 安全：vkernel框架必须确保不会为容器系统引入新的漏洞
3. 易部署：可以快速部署，无需重新编译内核或服务停机；不破坏现有的容器应用和环境，应与容器系统无缝集成，不需要对容器应用进行修改，让用户无感使用

![image-20250325131910549](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325131910549.png)

![image-20250325132847484](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325132847484.png)

基于 Rust 的轻量级虚拟内核为容器消除对内核代码与数据依赖；轻量级重定向技术拦截内核函数的访问为容器与虚拟内核的交互提供支持。

### 轻量级Rust内核模块设计

实现在内核态，比实现在用户态更具性能优势，而目前常见扩展内核功能的实现有四种，我们选择了第四种

1. 修改内核（扩展cgroups）
2. 可加载内核模块LKM
3. eBPF
4. Rust LKM

- **优势**
  - 前三种往往难以同时满足性能、安全性、易部署性这三条原则，而RustLKM可以同时满足

- **劣势**

  - 比eBPF技术安全性差一点，但比原生内核模块安全性高很多

- **vkernel框架**

  ![](C:\Users\Lenovo\Pictures\联想截图\联想截图_20250325140617.png)

  - Rust定制策略
    - 为容器引入私有内核数据，支持为容器私有化内存管理策略，可以根据不同容器的特定需求进行调整
  - Rust执行逻辑
    - 为容器引入私有内核代码，与Rust定制策略共同为容器提供具体内核函数服务
  - 扩展的Rust基础设施（内核层）
    - 为了支持Rust内核模块的运行，并提供必要的API和功能，使模块可以与内核进行交互
    - 包装了部分内核接口并暴露给Rust内核模块代码使用
    - 主要有三个包：`alloc`, `bindings`, `kernel`（对 `bindings` 的进一步封装，暴露给内核模块调用的Rust API接口）
  - 工作流程：
    - 基于 Rust 内核模块的虚拟内核框架由轻量级重定向技术支持，在内核调用具体内核服务执行内核函数时，由重定向组件将访问执行流重定向至虚拟内核，由 Rust 执行逻辑代码与容器的定制策略的数据协同提供具体服务，执行结束后退出虚拟内核，函数执行结束

### 轻量级重定向技术设计

设计基于 Rust 语言的轻量级重定向技术可以将容器对内核部分函数的访问重定向至vkernel，从而使私有化的内核数据能顺利地起作用，在实现时需要考虑两点：

- 性能：内存子系统使用非常频繁，需要将内核函数访问在vkernel和宿主机内核之间进行频繁重定向
- Rust与C的差异：从C语言跳转至Rust语言，需要考虑语言适配性问题

传统内核有两种重定向技术：

- 基于陷入的方法（即基于 trap）
  - 需要OS的支持，通过在被跟踪点插入断点指令，当该指令被执行时，中断处理程序将控制转移到指令代码区域
  - 具体有kprobes, ftrace, ptrace，经实验都带来了50%以上的额外开销
  - 缺点是开销较高
- 基于跳转的方法（本文采用的方法）
  - 通过简单的跳转指令（例如 call 指令）跳转到指定区域，而不需要触发 trap 指令
  - 比如inline hook 
  - 开销较低

**具体设计**

- 采用inline hook。在函数入口处插入一个 `jmp` 指令，并直接指向自定义函数的入口地址。当内核调用该函数时，会首先跳转到自定义函数的入口处执行自定义的代码，完成操作后再跳转回原函数继续执行。由于 Inline Hook 技术采用的是基于 `jmp` 指令的重定向技术，不需要保存和恢复寄存器状态，也不需要进行额外的栈操作，因此可以极大地减少重定向操作的开销

  ![image-20250325150510413](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325150510413.png)

- 然而，由于该方案直接采用 jmp 指令改变内核执行流，而函数头部通常会进行复杂的栈操作以及参数操作，而在函数返回点会进行返回值保存等操作，因此在设计 Inline Hook 时，**需要对虚拟内核中的新函数有两方面的限制**：
  - 新函数的参数需要与原函数完全一致，以保证函数头部栈操作完全一致
  - 新函数的返回值需要与原函数完全一致，以保证函数返回时寄存器值正确
- 考虑Rust与C的差异，为了保证基于 `jmp` 指令的执行流在不同语言代码间正确运行，需要对rust内核模块函数精心设计，主要两部分：
  - **原生函数名链接**：函数名修饰（Name mangling）一般是直接通过编译器完成，对用户透明。当前 Rust 默认情况下为了保证函数代码链接时函数名称的唯一性，会在编译时为函数名添加包信息，命名空间信息等。然而，在 Rust 语言函数名修饰后，重定向机制就无法通过函数名寻找到 Rust 内核模块中的目标函数地址，从而导致无法将执行流重定向至 Rust 内核模块。所以我们在内核模块中为所有 Rust函数启用 `no_mangle` 属性，保留原始函数名，从而与 C 语言执行流兼容
  - **兼容的** **Rust** **函数接口**：由于我们采用的重定向机制基于 `jmp` 指令，在内核源函数被调用后会直接跳转至内核模块中的 Rust 函数，所以面向 Rust 内核模块的重定向技术需要约定函数的寄存器使用规范与内核函数保持一致。所以，Rust 内核模块中的函数还需要使用另外一个约束：`extern "C"`。通过该约束，Rust 编译器会对 Rust 函数使用 C 语言的调用规范，从而保持与内核函数的兼容性




## 如何使用

### 准备

Vkernel 目前支持的平台有 Ubuntu 18.04、Ubuntu 20.04。其他 Linux 发行版暂未经过测试。

你需要提前安装以下工具：

- Docker
- Docker-slim
- Go
- Python3
- Make、Gcc 及其他编译内核的库

### 安装

**clone 仓库**

```bash
$ git clone https://github.com/huster-hh/vkernel.git
```

**安装 Linux 内核**，具体参考 Linux 内核编译安装方式

```bash
$ cd vkernel_kernel
```

**安装 vkernel 内核模块**

1. 如果你想给 vkernel 模块自定义 seccomp、apparmor 规则，可以继续往下，否则直接跳到**第 2 步**。

   先进入构建工具目录。

   ```bash
   $ cd vkernel/vkernel_builder
   ```

- 自定义 seccomp 规则。**（可选，推荐）**

  其中，`-i` 指定 seccomp 的 profile 文件（[示例](https://github.com/moby/moby/blob/master/profiles/seccomp/default.json)），`-o` 指定生成 seccomp 相关代码的目录，`-s` 指定 syscall.c 模板文件。

  ```bash
  $ python3 seccomp.py -i myseccomp.json -o ../vkernel_module -s ./input/syscall.c
  # 例如
  # python3 seccomp.py -i ./input/default.json -o ../vkernel_module -s ./input/syscall.c
  ```

- 自定义 apparmor 规则。**（可选，推荐）**

  其中，`-i` 指定 apparmor 的 profile 文件（[示例](https://github.com/moby/moby/blob/master/profiles/apparmor/template.go)），`-o` 指定生成 apparmor 相关代码的目录，`-v` 指定 apparmor.c 模板文件。

  ```bash
  $ python3 apparmor.py -i myapparmor -o ../vkernel_module -v ./input/apparmor.c
  # 例如
  # python3 apparmor.py -i ./input/docker-nginx -o ../vkernel_module -v ./input/apparmor.c
  ```

- 如果你不想通过上面两种方式自定义 seccomp 和 apparmor 规则，可以指定一个 `my.json` 配置文件，然后使用 **root** 用户运行如下命令，通过 docker-slim 生成相应规则。

  ```bash
  $ python3 main.py -i my.json -o ../vkernel_module -s ./input/syscall.c -v ./input/apparmor.c
  # 例如
  # sudo python3 main.py -i ./input/nginx.json -o ../vkernel_module -s ./input/syscall.c -v ./input/apparmor.c
  ```

  其中，`my.json` 格式如下：

  ```json
  {
        "name": "nginx",
        "image-url": "nginx",
        "label" : "",
        "opts": ""
  }
  ```

  其中，`name` 是自定义的名字，`image-url` 为 docker 镜像名，`label` 为 docker 镜像标签，`opt` 为 docker-slim 命令选项。

2. 进入 vkernel 模块目录，编译内核模块安装。

   ```bash
   $ cd vkernel_module
   $ make
   $ sudo make install
   ```

**安装 vkernel 运行时**

vkernel向上与容器运行时对接，要求安装支持vkernel功能的运行时

1. 编译生成 vkernel 运行时

   ```bash
   $ cd vkernel_runc
   $ make
   $ cp runc /usr/local/bin/vkernel-runtime
   ```

2. 给 docker 添加 vkernel 运行时

   编辑 `etc/docker/daemon.json`，添加如下内容：

   ```json
   {
        "runtimes": {
           "vkernel-runtime": {
               "path": "/usr/local/bin/vkernel-runtime"
           }
       }
   }
   ```

   重启 docker 引擎 `sudo systemctl restart docker`。

**使用**，启动容器时添加 `--runtime=vkernel-runtime` 参数，即按照常规容器运行时的方式配置docker/k8s等工具进行使用

```bash
$ docker run --rm --runtime=vkernel-runtime -itd ubuntu /bin/bash
265d5c39c6a882ca531e9b5ed2d3c4d305f13f142cc1c9cd50246221b592e55b
$ lsmod | grep vkernel
vkernel_265d5c39c6a8    40960  0
```

### 可选配置项

在k8s上部署vkernel时，首先要配置runtimeclass

```yaml
# runvk.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: runvk
handler: runvk

# .yaml
spec:
	runtimeClassName:runvk	# 表示使用一个特定的运行时类来运行容器
```

其次，支持在 `.yaml` 文件中通过  `metadata.annotations` 字段进行运行时配置和额外的安全配置，例如

```yaml
annotations:
  vkernel.syscall_profile: "/vkconfig/syscall.profile"	# 默认为容器路径
  # 如果需要指定主机路径需要添加.hostpath后缀
  # e.g.
  # vkernel.syscall_profile.hostpath: "/vkconfig/syscall.profile"
  vkernel.file_profile: "/vkconfig/file.profile"
  container.apparmor.security.beta.kubernetes.io/bench-httpserver-vk: unconfined
```

**系统调用规则**

profile格式与seccomp json格式相同

```yaml
annotations:
	syscall_profile.hostpath: "/path/to/syscall_profile"
```

**文件访问规则**

通过vkernel.file_profile配置文件访问规则，profile格式为：

```shell
# comment: deny write on /etc/hostname
deny /etc/hostname w

# comment: deny read/write on /sys/fs/cgroup
deny /sys/fs/cgroup rw
```

```yaml
annotations:
	file_profile.hostpath: "/path/to/file_profile"
```

**资源策略配置**

通过cpu_profile，memory_profile可以配置容器的CPU、内存资源策略，profile格式为json

CPU profile 示例：

```json
{
    "policy": "fifo",
    "rr_timeslice_us": 50000,
    "wakeup_gran_us": 5000
}
```

内存 profile 示例：

```json
{
    "thp_shmem_enabled": "advise",
    "thp_enabled": "always",
    "thp_defrag": "never",
    "thp_use_zero_page": 0
}
```

.yaml 配置：

```yaml
annotations:
	cpu_profile.hostpath: "/path/to/cpu_profile"
	memory_profile.hostpath: "/path/to/memory_profile"
```

**内核参数配置**

配置容器独立sysctl，profile格式为json，有些sysctl仍受到主机配置的影响，比如一些资源limit相关的，不能超过主机的limit上限

profile示例：

```json
{
    "fs": {
        "file-max": 9688404,
        "nr_open": 1048576,
        ...
    },
    "kernel": {
        "msgmax": 8192,
        "msgmnb": 16384,
        "msgmni": 32000,
        "msg_next_id": -1,
        ...
    },
    "net": {
        "nf_conntrack_max": 39316,
        "core.busy_poll": 1,
        ...
    }
    "vm": {
        "max_map_count": 65530,
        "mmap_min_addr": 65536,
        ...
    }
}
```

.yaml 配置：

```yaml
annotations:
	sysctl_profile.hostpath: "/path/to/sysctl_profile"
```

**自定义插件**

通过vkernel.custom配置自定义扩展，缺省参数是default。Vkernel内置了两种自定义扩展类型：

- default，缺省配置，对部分系统调用和文件访问进行限制
- analysis，用于负载分析，记录容器运行时所有使用的syscall和执行的程序，通过 `/sys/kernel/debug/vkernel/$ID/analysis` 呈现

Vkernel支持添加自定义规则，只需要按照规范编写并注册，后续创建的容器时就可以配置使用

## 适用场景

### 系统调用隔离

1. **问题背景**

BPF表达能力受限

- BPF在 Linux 内核中被用于安全监控（如 seccomp、eBPF），但它**无法直接解引用指针**，即不能解析系统调用参数指向的内存内容
- BPF 也不能直接**限制某个操作的频率**，如防止短时间内的高频调用
- 传统方法的局限性：
  - seccomp：主要用于**拦截和限制系统调用**，但它只能基于**数值参数**进行过滤，对于 `mount("cgroup", ...)` 这样的系统调用，**无法解析 "cgroup" 这个字符串**，所以无法阻止攻击者挂载 `cgroup` 并利用它进行逃逸
  - 内核补丁：直接在 Linux 内核中打补丁，修改 `mount` 逻辑，虽然可以解决问题，但：如果补丁影响整个 Linux 机器，而不仅仅是容器，那可能会带来兼容性问题；需要考虑各种情况，例如不同的挂载类型、权限管理等，维护成本较高

2. **业务场景**

系统调用间接参数过滤

- 系统调用是用户态进程与内核交互的主要方式，而某些攻击手法会滥用系统调用来进行安全逃逸
- 例如，攻击者可能利用 `mount` 系统调用挂载 `cgroup`，并通过特定方式逃逸出容器环境（如 `mount-cgroup` 逃逸漏洞 https://github.com/cdk-team/CDK/wiki/Exploit:-mount-cgroup）
- 而 `mount` 调用的参数通常是**指针**，指向一个字符串（如 `"cgroup"`），BPF 由于**无法解引用指针**，就无法读取这些字符串进行判断和过滤。

3. **vkernel 的优势**

- 可以解引用指针，直接读取指针参数
- 运行在容器级别，旨在容器内部生效，不会影响宿主机的其他系统调用行为，避免了全局影响
- 更灵活，可以实现针对系统调用的细粒度过滤，而不受 BPF 的表达能力限制

### 文件访存隔离

1. **问题背景**

apparmor是linux内核的MAC机制，通过基于路径的访问控制，限制进程对文件、网络、进程操作等资源的访问权限，然而在某些场景下会有**显著开销**，比如系统调用开销（每次文件访问都要检查apparmor配置，可能影响高频I/O任务的性能）、路径解析开销（路径解析和字符串匹配影响文件访问的吞吐率） p.s.这几个场景不是很典型

2. **业务场景**

3. **vkernel 的优势**

### 内核数据隔离

1. **问题背景**

日志可能泄露敏感信息

2. **业务场景**

日志可能包含地址信息，使得攻击者更易利用漏洞

### 内核参数隔离-内核资源限额

1. **问题背景**

一些系统资源总量有限，恶意或失控容器占用过多甚至耗尽，会影响其他业务负载，例如：

- file-max,nr_open,mount-max,
- pipe-max-size,pipe-user-pages-hard,pipe-user-pages-hard,pipe-user-pages-soft
- threads-max
- max_map_count

2. **业务场景**

最大打开文件数设置过高可能会异常地占用过多内存

https://cloud.tencent.com/developer/article/2429855

### 内核参数隔离-冲突负载并存

1. **问题背景**

不同负载对某些系统参数配置要求可能存在冲突，例如：overcommit_{memory, kbytes, ratio}
一些负载处于fork或oom的控制考虑，对overcommit_memory的行为有要求

2. **业务场景**

一些数据库类的负载对overcommit_memory有配置需求，大概分为两类：

- 偏好1/ALWAYS，场景是利用 `fork` 进行备份，仅分配了虚拟地址空间，但不实际消耗内存，因而希望确保 `fork` 总是成功
- 偏好2/NEVER，场景是在（并发度过多导致）内存紧张时，倾向于应用自己进行报错处理，而不是直接被系统OOM-killer杀掉。通常内存使用模式都是1)先检查 `mmap`，2)然后再实际分配内存，设置为2/NEVER时在第1)就可以发现问题自行处理，显著降低被OOM kill的可能性

附几个链接：
Redis (prefers 1): https://redis.io/docs/latest/operate/oss_and_stack/management/admin/#linux
PostgreSQL (prefers 2): https://www.postgresql.org/docs/current/kernel-resources.html#LINUX-MEMORY-OVERCOMMIT
Greenplum (prefers 2): https://techdocs.broadcom.com/us/en/vmware-tanzu/data-solutions/tanzu-greenplum/7/greenplum-database/best_practices-sysconfig.html

附几个用例：
redis配置不为1/ALWAYS时会报WARNING

```bash
1:C 30 Dec 2024 06:16:08.118 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
```

postgresql在配置为1/ALWAYS时，并发度过高会被OOM kill；配置为2/NEVER时，分配虚拟地址空间报错但进程不会被OOM kill

```bash
[ 1703.082551] Memory cgroup out of memory: Killed process 75284 (postgres) total-vm:314172kB, anon-rss:2976kB, file-rss:8448kB, shmem-rss:8064kB, UID:999 pgtables:240kB oom_score_adj:-997
[ 1703.083981] postgres invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=-997
```

```bash
[ 1874.738152] __vm_enough_memory: pid: 107431, comm: postgres, not enough memory for the allocation
[ 1874.739823] __vm_enough_memory: pid: 107431, comm: postgres, not enough memory for the allocation
[ 1874.896089] __vm_enough_memory: pid: 107431, comm: postgres, not enough memory for the allocation
[ 1874.897966] __vm_enough_memory: pid: 107431, comm: postgres, not enough memory for the allocation
```

### 内核参数隔离-特定负载调优

1. **问题背景**

特定的资源配置策略可能只适用于部分负载，例如：transparent_hugepage
透明大页对于访存密集、集中使用大块内存的负载能提升性能，但对于其他负载可能会带来内存浪费

2. **业务场景**

mysql受益于透明大页访存，可单独设置启用透明大页；而redis则推荐关闭此功能



## 基于vKernel的容器内存管理策略定制化方法

### 项目背景

- 容器技术共享宿主机内核，带来了轻量化的优点，但也导致其存在**隔离性不足、定制性弱**的问题，尤其在多租户应用混布场景下，容器技术很难满足不同应用对内核机制的个性化定制需求，严重限制了容器应用性能发挥
- 由于容器内部运行的应用不同，内存使用模式也不尽相同，**对内存管理策略有不同的定制化需求**（如：mysql受益于透明大页访存，可单独设置启用透明大页，而redis则推荐关闭此功能；部分容器应用如redis依赖内存超配机制，而部分如postgresql在物理内存紧张时需要禁用内存超配机制增强稳定性），然而memory cgroup**作用于物理内存一侧**为应用提供限制隔离功能，无法隔离下层物理内存组织模式的策略，也缺少对管理上层虚拟内存分配的虚拟内存超配策略隔离的支持，所以它存在很大局限性
- 基于现有容器框架扩展对物理内存、虚拟内存个性化策略的支持需要大幅修改内核代码，存在安全性的问题，并且内核多版本间扩展性不足，部署需要重启内核，难度较大
- 缺乏一个安全、轻量且可扩展的统一框架来增强容器隔离性，以支持容器内存管理策略的定制化
- 因此，为了更好地满足不同容器应用在不同场景下的需求，需要针对性地对内存管理策略进行个性化定制和优化，从而提高容器性能、可靠性，同时还要提高隔离性，避免容器间修改宿主机管理策略造成的相互影响

### 项目效果

- 提出了一种轻量级虚拟内核框架——vkernel，让容器应用在基本不损失性能的情况下构建独立虚拟内核，摆脱对宿主机内核的依赖
- 基于该框架，为容器应用构建了包含独立内存管理策略的vkernel实例，实现了定制化的透明大页和虚拟内存管理策略，使容器应用可以根据自身的特点以及不同运行环境，选择最适合自己的内存管理策略，从而可以发挥最佳的应用性能
- 实验分别测试了容器内存管理策略定制化系统的轻量化特性和定制化效果
  - 相较于原生容器，该系统仅带来低于3%的额外内存访问时间开销和低于1%的容器应用吞吐率开销
  - 通过为容器应用定制透明大页管理策略，容器密集型内存访问性能提升230%
  - 通过为容器应用定制虚拟内存管理策略，容器应用吞吐率最高提升24%

### 为何采用Rust

- 内核模块用rust编写，轻量级重定向函数是面向rust函数的
- 内核态的方案与轻量级重定向技术使得vkernel不会影响容器的轻量化特性
- Rust 的特性能防止常见的编程错误，防止空指针或悬浮指针引用，减少了vkernel的安全隐患
- 性能出色，采用零开销抽象概念简化语言设计、没有GC，故没有额外运行时开销
- 基于Rust开发的内核模块基础设施已成功融入Linux内核主线

### 概念解释

#### 容器内存管理

1. 内存子系统

   操作系统内存管理机制向下管理全局物理内存，向上为应用提供虚拟地址空间，并通过页表管理虚拟地址到物理地址的

   映射；内存管理中通常存在不同的管理策略

   - **物理内存**
     - 由内核管理完成初始化、分配、释放和回收等过程
     - 管理策略支持更大的物理页的透明大页（THP）策略
   - **虚拟内存**
     - 面向进程的具体使用，由页表完成虚拟内存到物理内存的映射，实现进程间透明地共享使用物理内存
     - 管理策略支持虚拟内存超配策略，以控制虚拟内存分配行为

2. 简介

   容器应用共享宿主机内核，从而共享宿主内核的内存管理策略。容器内进程使用虚拟地址，并通过页表映射为物理地址。

   容器用cgroups细粒度管理和控制硬件资源，其中 **memory cgroup** 用于为容器内存管理提供底层支持，即作用于物理内存侧

   - **memory cgroup** 
     - 通过统计和限制进程组物理内存分配，控制容器使用的物理内存量保持在一定范围
     - 当容器尝试分配超出限制的物理内存时，内核会采取应对措施，如触发容器内存回收、终止进程等
     - 避免了内存硬件虚拟化层的开销，有效保证了容器应用运行性能

#### 物理内存-THP 透明大页策略

1. **定义**

   - 传统 Linux 内核默认使用 **4KB 小页（normal page）** 进行内存管理，但计算机物理内存大小不断增大，4KB的页表内存开销、TLB缓存压力和地址转换开销也会相应增大
   - 现代 CPU 支持 **大页（huge page）**（将连续的普通内存页面组合成更大的页面，如 2MB、1GB），可减少 TLB 开销，提高内存访问效率
   - THP 允许 Linux **自动将小页合并为大页**，提高 TLB 命中率，减少页表查找的开销，提高性能
   - THP 机制默认是 **异步合并**（内核后台线程 `khugepaged` 自动合并小页）和 **即时分配**（缺页时直接分配大页）

2. **好处**

   - 消除了页表一个层级，减少了地址转换与页表内存开销
   - 扩大了有限 TLB 缓存条目的内存覆盖范围，减少了缓存压力
   - 降低虚实地址转换的开销，对于内存访问密集容器应用有很大的性能提升（在4KB访问粒度下，THP使容器内存访问时间减少70%）

3. **缺点**

   - 会增加单次缺页中断的延迟，内存碎片化程度较高时，延迟更明显（在2MB访问粒度下，THP使容器内存访问时间增加近80倍），所以对于某些延迟敏感的应用来说是无法接受的，如Redis和MongoDB

4. **实现**

   对大页的支持同时需要软件支持和硬件支持

   - **硬件支持**
     - 在页表硬件设计中实现，比如再64位Intel处理器下，使用页表页目录项（PDE）的第7位PS标记，如果PS=0，表示PDE映射一个页表，页表包含512个页表项（PTE），每个PTE映射一个4KB的页，如果PS=1，表示PDE映射一个大页
   - **软件支持**
     - 在OS中实现支持，有两种方法：
       - 在启动时预留一个连续的大页缓冲池，应用程序可以自行在代码中决定是否使用大页（不够灵活）
       - **透明大页机制**，OS自动管理分配大页，不需要应用显示参与大页的分配与代码修改（更实用、广泛接受）
       
     - 目前 Linux 透明大页机制**只作用于匿名页**，并通过两种方式为进程分配使用大页，总结如图：
     
       - 进程访问虚拟地址，触发缺页中断时，如果该虚拟地址为一个连续的大页面，OS立刻分配对应的连续物理大页，如果分配失败，OS会执行内存碎片化整理，通过页面迁移创建一个空闲的物理大页；这个过程会阻塞缺页中断过程，增大缺页延迟，在内存碎片化严重时，页面迁移仍然可能分配不出连续的物理大页。此外，在大页分配结束后，会被立刻初始化，**这个过程增大了第一次缺页中断延迟，但可以减少后续内存访问的平均延迟**
     
       - Linux 内核使用一个内核线程 `khugepaged`( Linux 中自动合并小页面为透明大页的后台线程)，异步扫描系统上的页表，当发现一个连续的 2MB 匿名虚拟内存区域
     
         并且包含至少一个 4KB 映射后，该线程会尝试为该虚拟内存区域分配并映射一个物理大页，如果分配失败则会触发内存压缩机制，在分配出物理大页时，该线程会把包含的 4KB 页迁移到分配的物理大页上。在 `khugepaged` 线程作用时会阻塞进程对该虚拟地址区域的访问
     
         ![1742823022786](E:\Tencent\WeChat\Wechat Files\WeChat Files\wxid_bgsbmjszjqho22\FileStorage\Temp\1742823022786.png)

5. **为什么 MySQL 受益于透明大页**

   MySQL 的查询缓存、索引、InnoDB Buffer Pool 等组件大多是**长期访问的大数据块**，能充分利用大页带来的 TLB 提高，具体解释：

   - **访问模式更适配大页**：顺序读写，内存访问局部性强，使用大页减少TLB miss，提升查询性能
   - **大页适用于长期驻留的内存**：MySQL分配的大量buffer pool一般长期存在，不会频繁释放和重新分配，THP适合**一次分配长期使用**的场景

6. **为什么Redis推荐关闭透明大页？**

   Redis是基于内存的高性能 Key-Value 数据库，其访问模式与 MySQL 不同，THP可能导致性能下降，主要原因包括：

   - **Redis访问模式随机**：Redis 主要处理**小对象**，其kv存储模式**访问内存位置较分散**，不像 MySQL 是顺序扫描的大块数据；而且TLB miss 并不是 Redis 性能的主要瓶颈，使用 THP 反而可能导致 **额外的内存管理开销**
   - **THP 可能导致高 `khugepaged` CPU 开销**：Redis 频繁申请和释放小块内存，导致 `khugepaged` 线程不断合并和拆分页面，会造成额外的 CPU 消耗，影响 Redis 的请求处理能力
   - **影响 fork 性能**：Redis 在执行 **BGSAVE 或 AOF 持久化** 时，会使用 `fork()` 生成子进程，而 THP 导致写时复制（COW）开销增加，`fork()` 复制时，父子进程共享 THP 2MB 页，而**只要有一小块数据修改，就需要复制整个 2MB 页面**，导致**额外的内存占用和拷贝开销**，这在高并发写入的场景下显著降低 Redis 性能

7. **总结**

   - 是否启用 THP 需要考虑**工作负载的不同**，涉及 **内存访问模式、页面合并开销、内存碎片化、TLB 命中率**
   - 访存密集型应用可以通过THP机制显著提高性能
   - 延迟敏感型应用则倾向于禁用THP机制，避免引起高延迟峰值

#### 虚拟内存-超配策略

1. **定义**

   - 进程拥有独立的虚拟地址空间，按理说虚拟内存可以无限使用，然而为了限制进程过度使用虚拟内存、避免出现频繁的内存回收与内存交换，OS提供了虚拟内存配置策略，即**虚拟内存超配机制**
   - 允许用户为参数 `overcommit_memory` 配置不同策略：
     - 启发式策略（配置为 0），拒绝一次性分配虚拟内存超过物理内存量的行为
     - 始终允许超配策略（配置为 1），允许所有虚拟内存分配行为
     - 禁止超配策略（配置为 2），只允许虚拟内存分配一定量

2. **好处**

3. **缺点**

   - 在多租户容器环境下，不同容器应用共享宿主内核机制，容器缺少独立的虚拟内存管理机制，无法个性化定制自身虚拟内存管理策略。更为严重的是，在宿主机限制系统虚拟内存总量时，所有容器共享这些虚拟内存限制，如果存在恶意容器过度占据虚拟内存，会严重影响应用的性能与稳定

4. **实现**

5. **为什么Redis应用要将 `overcommit_memory` 设置为1**

   Redis 数据通常存储在内存中，在处理大量数据时，会占用大量内存资源。为了避免系统崩溃或数据丢失等问题，Redis 会基于后台保存（background save，bgsave）机制备份数据。该机制在运行时通过对主进程执行 `fork` 产生一个子进程进行备份操作，然而 `fork` 底层依赖写时复制机制，产生的子进程具有独立的虚拟内存，而与父进程共享相同的物理内存，从而出现虚拟内存使用量大大超过物理内存使用量的情况。为了应对这种情况，Redis 倾向于将操作系统虚拟内存管理策略调整为始终超配

   （`overcommit_memory` 参数 1），从而使得后台备份机制可以正常执行，更好地保证数据完整性。

6. **为什么Postgresql应用要将 `overcommit_memory` 设置为0**

   - 预分配的虚拟内存空间并不是实际可直接使用的物理内存，所以在实际需要使用时，可能会出现物理内存不足，从而导致容器应用实例崩溃或系统变得不稳定
   - Postgresql 在虚拟内存超配的情况下，内存紧张时其服务管理进程会因 OOM 被系统终止，导致数据库停止运行，服务稳定性大幅降低。官方文档中明确指出，为了解决该问题可将虚拟内存超配机制关闭。因为在关闭内存超配机制时，分配过量的虚拟内存会失败（例如 `malloc` 函数返回空），而程序内部如果正确处理这种情况，就可以尽可能地保证不会出现物理内存不足的情况，从而保证程序运行的稳定性

### 具体实现

Linux 6.1,Docker20.10.13

#### 框架

![vkernel架构图](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325151936500.png)

1. **Rust中间层组件**：

   - 用于扩展 Rust 内核模块的基础设施，为vkernel提供基础支持，需要扩展  `bingdings` 与 `kernel` Rust包
   - `bindings` 的主要功能是自动生成 FFI 绑定，以便调用C内核代码，包括：
     - 解析内核头文件中的结构体定义
     - 解析 `.h` 中声明、 `.c` 中定义的非内联函数
     - 处理内联函数（通常定义在头文件中，并在编译时展开）
   - 对于非内联函数，只需为`bindgen` 指定对应头文件，即可自动生成对应的 Rust API
   - 对于内联函数，它在编译时被展开到调用函数中间，非直接调用模式，**不会生成独立的符号可供链接**，无法直接生成对应接口
     - 解决方法：使用非内联包装函数调用内联函数，然后Rust通过FFI绑定包装函数
   - `kernel` 是对 `bindings` 生成的内核 Rust API 的包装，主要包括内核定义的结构体，以及部分内核函数接口
   - 几个典型的面向 Rust 内核模块导出的内核结构定义与函数如下表所示：

   | 导出符号名                  | 类型     | 需求来源     | 功能介绍                      |
   | --------------------------- | -------- | ------------ | ----------------------------- |
   | mem_cgroup                  | 结构体   | 策略定制组件 | 容器 Memory Cgroup 的结构定义 |
   | text_poke                   | 函数     | 重定向组件   | 动态修改内核函数指令          |
   | insn_get_length             | 函数     | 重定向组件   | 获取函数指令长度              |
   | task_active_pid_ns          | 函数     | 策略定制组件 | 获取进程的 pid Namespace      |
   | percpu_counter_sum_positive | 内联函数 | 策略定制组件 | 获取 per cpu 变量的值         |

2. **重定向组件**：

   - 基于 `jmp` 指令的inline hook技术实现内核函数重定向机制，将容器对宿主内核的部分访问重定向到vkernel，让容器使用自己的私有化内核代码和数据

   - 将宿主机内核函数跳转到vkernel中的对应函数，如果某些进程（如宿主机进程）不应该被重定向，则使用回跳机制恢复执行流

   - 过程中分为**三个核心步骤**：

     - **构建跳转指令**：

       - 采用 `0xE9` 相对跳转指令，`jmp` 指令长度固定为5字节（1字节 `0xE9` + 4字节偏移量），偏移量=目标函数地址-当前指令的下一条地址，然后用 `jmp` 指令替换掉源函数的第一条指令

     - **构造回跳指令**：

       - `jmp` 会拦截所有对内核源函数的调用，但宿主机进程仍需要执行原始逻辑，因此需要提供回跳机制

       - 为避免无限递归，在 vkernel 中构造一个垫片函数，其中第一条指令恢复源函数中被覆盖的指令、第二条指令是 `jmp` 跳回源函数的第二条指令，继续执行剩余代码，be like:

         ```
         trampoline_function:
           original_first_instruction  ; 恢复被覆盖的第一条指令
           jmp kernel_function+5        ; 跳回 kernel_function 的第二条指令
         
         if !来自容器 {
             trampoline_function();  // 回跳到原始内核代码
         }
         ```

     - **指令替换**:

       - 替换内核源函数的第一条指令,，确保所有执行流都被劫持
       - `jmp` 指令为 5 字节，但是源函数首条指令不一定为 5 字节，所以要用 `insn_get_length` 计算原始指令长度接口并保存原始指令用于回跳
         - 多核下会有问题，解决方案是通过 `stop_machine` 暂停所有CPU核，然后用 `text_poke` 接口把前五个字节指令替换。并刷新TLB确保缓存一致性，替换完成后解除暂停

   ![image-20250325154403669](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325154403669.png)

3. **全局管理组件**：

   - 管理vkernel实例生命周期，即在容器启动/销毁时，**创建/释放与容器关联的策略定制组件**
   - 核心机制是用 `vector` 实现的**拉链式哈希表**，键是容器的 `pid_namespace` 结构地址，值是容器的虚拟策略定制结构体地址，使用拉链法，即同一个哈希桶可以存放多个策略定制组件结构体，从而解决哈希冲突的问题
     - **当容器启动时**
       1. 组件会为该容器生成一个 **策略定制组件**（policy customization component）
       2. 该组件会被 **绑定到容器的 `pid_namespace`（进程 ID 命名空间）**，确保该命名空间内的进程都能使用相应的 **虚拟内核策略** 进行内存管理
     - **当容器销毁时**
       1. 组件会 **释放该容器的策略定制组件**，以回收资源
       2. 从全局哈希表中 **移除该 `pid_namespace` 的映射**，确保不会出现内存泄漏
   - 为了能在容器启动和销毁时进行管理，组件需要检测和拦截 `pid_namespace` 的创建和销毁函数：
     - 拦截 `create_pid_namespace` 
       1. 该函数在创建 `pid_namespace` 时被调用
       2. 组件检测到该函数执行时，会初始化一个对应的策略定制组件结构体
       3. 将结构体地址添加到全局哈希表
     - 拦截 `destroy_pid_namespace` 
       1. 该函数在 `pid_namespace` 被销毁时调用
       2. 组件检测到该函数执行时，会释放对应的策略定制组件结构体，回收资源
       3. 删除全局哈希表中的映射

4. **策略定制组件**：

   - 负责**为容器提供个性化的内存管理策略**，并与宿主机的内存管理机制协同工作，主要包含两个部分：
     - 内存管理参数结构体：包含容器内存管理所需的各种**私有参数**，如内存分配策略、缓存管理、垃圾回收策略
     - 执行逻辑：内核原执行逻辑的Rust版本，但使用私有内存管理参数，确保容器独立性；且执行过程中需要过滤掉非容器来源的执行流
   - 容器执行流有两类：
     - 容器进程直接调用（大多数，如容器内进程直接 `malloc` 或其他内存分配函数）
       - 识别方法：先通过 `current` 获取当前进程的 `PID`，然后通过 `task_active_pid_ns` 获取它的 `pid_namespace`，然后再全局管理组件的哈希表中查找是否存在，若在则说明该进程属于容器，可以应用定制策略
     - 内核线程处理容器内存资源（宿主机的内核线程在处理容器内的进程资源，如 `khugepaged`）
       - 该线程不属于容器，因此不能直接根据 PID 判断
       - 识别方法：根据虚拟地址区域反向查找进程，获取进程的 `pid_namespace` 并判断是否属于容器

### 遇到的问题

1. **容器策略配置冲突问题**

   - 容器在定制自身内存管理策略时，往往会遭遇与宿主机内存管理策略不一致的问题，尤其是当容器的策略配置超过宿主机时，这种冲突会更加严重。例如，当容器应用开启自身虚拟内存超配机制，而宿主机却禁用虚拟内存超配，则会引发严重的冲突
   - **解决方案：**
     - 提出**基于完全隔离思想的容器内存管理机制**，基本思想是基于虚拟机独立内核的特点，为每个容器提供一套完全独立的内存管理机制
       - 对于独立的THP机制要支持容器粒度：
         - 硬件支持会根据THP策略参数决定PS标记位，所以实现软件支持后无需再改动硬件
         - 复用宿主机的缺页中断处理逻辑，只需要对容器的缺页中断中执行THP策略判断的逻辑重定向
         - 采用**khugepaged 常驻线程**来支持大页合并，确保灵活性。
       - 独立的虚拟内存管理机制：
         - 提供独立的参数和审计机制，使得每个容器的虚拟内存管理互不影响

2. **容器共享内存决策问题**

   - 多个容器共存于同一宿主机之上，物理内存是全局共享的，容器定制化自身内存管理策略时，容器之间共享的内存会面临配置冲突的问题。例如，当容器之间出现内存页面共享，且该页面执行大页合并时，如果共享该页面的容器对透明大页有不一样的配置，会产生严重的冲突。

   - 一方面，THP只作用于匿名页，which在进程间存在共享，如 `fork` 产生的子进程与父进程间匿名内存共享；另一方面，社区在推进对文件缓存的THP支持，which使容器间页面共享更频繁

   - **解决方案**

     - 设计一种**基于投票的容器内存共享冲突决策机制**，在容器环境下，某个进程首次分配一个时，会被物理内存审计机制统计到容器对应的 `Memory Cgroup` 中，对于容器间共享的页面，内核基于**先分配者**统计机制，后续使用该物理页面的不会统计进其 `Memory Cgroup`

     - 我们采用统计者优先原则，即哪个容器的 `Memory Cgroup` 统计了该页面，它的个性化策略就具有更高的权重，投票流程：

       1. `khugepaged` 线程探测THP机会：
          - 当 `khugepaged` 发现开启THP的容器进程包含512 个连续的 4KB 小页面的虚拟地址区域时，尝试执行THP合并操作

       2. 遍历该区域的所有 4KB 虚拟页面：
          - 通过页表找到其对应的4KB物理页面

       3. 获取物理页面的归属容器：
          - 获取对该物理页面统计计费的所有者，并根据所有者的 `Memory Cgroup` 查找对应容器
          - 通过容器的 `pid_namespace` 访问其**虚拟内核配置**，获取该容器的 THP 设定，并为对应策略投票

       4. 为不同策略投票：

          - 统计所有物理页面的**归属容器**的 THP 策略

          - 进行**投票**，根据各容器的策略计算支持和反对 THP 的票数，票数多者取胜

   - p.s. 算法中需要根据虚拟地址->物理页，要用到进程页表，而 `khugepaged` 页面合并逻辑中同样需要依赖进程页表（查找虚拟地址对应的页中间目录PMD执行页面合并），所以算法中页表查找过程可以和原始逻辑耦合，不会带来过大的页表查找开销

3. **容器内存管理策略自适应问题**

   - 不同容器具有不同的内存使用模式，它们的透明大页策略和虚拟内存超配策略需求在不同的场景和运行阶段往往不同。因此，需要支持容器自适应地调整内存管理策略，以最大限度地发挥应用性能。

   - **解决方案**

     - 设计用户态策略定制组件，动态监测vkernel提供的容器应用内存指标，为容器动态指定特定的管理参数

       ![image-20250326102832863](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250326102832863.png)

     - （这块不讲了）
